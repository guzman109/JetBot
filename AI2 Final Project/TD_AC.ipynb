{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Carlos Guzman\n",
        "## AI2 Final Project \n",
        "## TD-AC"
      ],
      "metadata": {
        "id": "KRKQh2Wk0ZAg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKr49vRz0QWJ"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import random\n",
        "import serial\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from CSI_Camera import CSI_Camera\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gU6hTlR0QWL"
      },
      "outputs": [],
      "source": [
        "class Environment(object):\n",
        "    def __init__(self):\n",
        "        self.camera = CSI_Camera(1)\n",
        "        self.time_last_action = 0\n",
        "        self.last_action = None\n",
        "        self.last_frame = None\n",
        "        self.spin_pen = 0\n",
        "        self.ACTION_MAP = {0:'s', 1:'f', 2:'b', 3:'l', 4:'r'}\n",
        "    def reset(self):\n",
        "        # Check if cameras are opened.\n",
        "        if not self.camera.isRunning() or not self.camera.isOpened():\n",
        "            # Open and start camera\n",
        "            self.camera.open()\n",
        "            self.camera.start()\n",
        "\n",
        "        # Initialize variables\n",
        "        self.time_last_action = 0\n",
        "        self.last_action = None\n",
        "\n",
        "        # Read frame from camera\n",
        "        _, frame = self.camera.read()\n",
        "\n",
        "        self.last_frame = frame\n",
        "        return self.get_state(frame)\n",
        "    def close(self):\n",
        "        # Stop the camera and close them\n",
        "        if not self.camera.isOpened():\n",
        "            self.camera.stop()\n",
        "            self.camera.release()\n",
        "    def get_state(self, frame):\n",
        "        # frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY, dst=None)\n",
        "        frame = cv.resize(frame, (240, 240), dst=None, interpolation=cv.INTER_CUBIC)\n",
        "        state = T.ToTensor()(frame).unsqueeze(0)\n",
        "        return state\n",
        "    def step(self, action):\n",
        "        try:\n",
        "            action = self.ACTION_MAP[action]\n",
        "            # reset if new action.\n",
        "            if self.last_action != action:\n",
        "                self.time_last_action = 1\n",
        "\n",
        "            # Read new frame from camera\n",
        "            _, frame = self.camera.read()\n",
        "            state = self.get_state(frame)\n",
        "\n",
        "            # Calculate reward of the last (state, action)\n",
        "            reward, colision = self.reward(action, frame)\n",
        "\n",
        "            # Accumlate time since executing same action.\n",
        "            # Reset if new action is executed\n",
        "            self.time_last_action += 1\n",
        "            \n",
        "            # Update last_action and last_frame\n",
        "            self.last_action = action\n",
        "            self.last_frame = frame\n",
        "\n",
        "            return state, reward, colision\n",
        "        except RuntimeError:\n",
        "            print(\"Error: Camera not opened.\")           \n",
        "    def reward(self, action, frame):\n",
        "        collision = False\n",
        "        # if self.last_action == action:\n",
        "        if action != 's':\n",
        "            # Check for a collision by comparing the current frame and the last frame. \n",
        "            # Via the histograms comparison. \n",
        "\n",
        "            # Convert to HSV color space\n",
        "            last_frame_hsv = cv.cvtColor(self.last_frame, cv.COLOR_BGR2HSV)\n",
        "            frame_hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
        "            \n",
        "            # Histogram parameters\n",
        "            hist_size = [50, 60]\n",
        "            ranges = [0, 180, 0, 256]\n",
        "            channels = [0, 1]\n",
        "\n",
        "            # Calculate the Histogram of the last frame and normalize it.\n",
        "            hist_last_frame = cv.calcHist([last_frame_hsv], channels, None, hist_size, ranges, accumulate=False)\n",
        "            cv.normalize(hist_last_frame, hist_last_frame, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
        "            \n",
        "            # Calculate the Histogram of the current frame and normalize it.\n",
        "            hist_frame = cv.calcHist([frame_hsv], channels, None, hist_size, ranges, accumulate=False)\n",
        "            cv.normalize(hist_frame, hist_frame, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
        "\n",
        "            # Calculate the correlation coefficient between the histograms\n",
        "            correlation = cv.compareHist(hist_last_frame, hist_frame, 0)\n",
        "            # If correlation is higher than 0.99 then we can say that it's a collision with a barrier.\n",
        "            if correlation >= 0.99:\n",
        "                # Collision\n",
        "                collision = True\n",
        "                reward = -10\n",
        "        if not collision:\n",
        "            if action == 's':\n",
        "                # Reward for stopping\n",
        "                reward = max(-5, -1 * self.time_last_action)\n",
        "            elif action in ['l', 'r']:\n",
        "                reward = 1\n",
        "            elif action in ['f','b']:\n",
        "                # Going forward or backwards\n",
        "                reward = min(10, 2 * self.time_last_action)\n",
        "        return reward, collision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ4dOnd40QWN"
      },
      "outputs": [],
      "source": [
        "class ActorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorLoss, self).__init__()\n",
        "    def forward(self, action_log_probs, advantage):\n",
        "        # TD Error\n",
        "        return torch.mean(-action_log_probs * advantage)\n",
        "        \n",
        "class ActorCriticLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActorCriticLoss, self).__init__()\n",
        "        self.actor_criterion = ActorLoss()\n",
        "        self.critic_criterion = nn.HuberLoss(reduction='mean')\n",
        "    def forward(self, action_prob, next_value, value, reward, policy_entropy, beta=0.001, discount=0.99):\n",
        "        # Calculate State Action Value\n",
        "        q_value = (reward + discount * next_value)\n",
        "        # Estimate TD Advantage\n",
        "        advantage = q_value - value\n",
        "        #Calculate Log Action Probabilites \n",
        "        action_log_probs = torch.log(action_prob)\n",
        "\n",
        "        # Actor Loss (TD Error)\n",
        "        loss_actor = self.actor_criterion(action_log_probs, advantage).to(torch.float)\n",
        "        # Critic Loss (Huber)\n",
        "        loss_critic = self.critic_criterion(value, q_value).to(torch.float)\n",
        "        # Use Entropy for Regularization. Promotes Exploration!\n",
        "        regularizer = (policy_entropy * beta)\n",
        "        return loss_actor + loss_critic + regularizer\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, num_actions):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.CNN = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=3),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=16,out_channels=16,kernel_size=5,stride=3),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=1152, out_features=288),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(in_features=288, out_features=36),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.Actor = nn.Sequential(\n",
        "            nn.Linear(in_features=36, out_features=num_actions),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "        self.Critic = nn.Sequential(\n",
        "            nn.Linear(in_features=36, out_features=1)\n",
        "        )\n",
        "    def forward(self,state):\n",
        "        features = self.CNN(state)\n",
        "        \n",
        "        value = self.Critic(features)\n",
        "        policy = self.Actor(features)\n",
        "\n",
        "        return value, policy   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjBWtjLI0QWQ"
      },
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "    def __init__(self, LEARNING_RATE=25e-4):\n",
        "        self.ACTION_MAP = {0:'s', 1:'f', 2:'b', 3:'l', 4:'r', 5:'(', 6:')'}\n",
        "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.HISTORY = {'Returns': [], 'Collisions': [], 'Actions': []}\n",
        "\n",
        "        self.driver = serial.Serial(\"/dev/ttyTHS1\", 9600, timeout=1)\n",
        "        self.driver.reset_input_buffer()\n",
        "        self.last_action = None\n",
        "\n",
        "        self.model = ActorCritic(5).to(self.DEVICE)\n",
        "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.criterion = ActorCriticLoss()\n",
        "\n",
        "    \n",
        "    def call_actor_critic(self, state):\n",
        "        value, policy = agent.model(state.to(self.DEVICE))\n",
        "        return value, policy\n",
        "\n",
        "    def select_action(self, policy):\n",
        "        policy_dist = torch.distributions.Categorical(policy.view(-1))\n",
        "        action = policy_dist.sample().item()\n",
        "        return action\n",
        "\n",
        "    def execute_action(self, action):\n",
        "        action = self.ACTION_MAP[action]\n",
        "        if action in ['f','b'] and self.last_action in ['f','b'] and self.last_action != action:\n",
        "            # Have to stop first to go opposite direction\n",
        "            self.driver.write('s'.encode('ascii'))\n",
        "            time.sleep(0.5)\n",
        "        if action in ['l', 'r']:\n",
        "            # Turning Left or Right\n",
        "            self.driver.write('s'.encode('ascii'))\n",
        "            time.sleep(0.5)\n",
        "            self.driver.write(action.encode('ascii'))\n",
        "        else:\n",
        "            self.driver.write(action.encode('ascii'))\n",
        "            time.sleep(0.5)\n",
        "        self.last_action = action\n",
        "\n",
        "    def wait_for_next_epsiode(self):\n",
        "        spin = self.ACTION_MAP[random.randint(5,6)]\n",
        "        self.driver.write('s'.encode('ascii'))\n",
        "        time.sleep(1/1000 * 8)\n",
        "        self.driver.write(spin.encode('ascii'))\n",
        "    \n",
        "    def optimize_policy(self, action_prob, next_value, value, reward, policy_entropy):    \n",
        "        # Calculate Loss\n",
        "        loss = self.criterion(action_prob, next_value, value, reward, policy_entropy)\n",
        "\n",
        "        # Backprop & weight update\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnF3QxYB0QWR"
      },
      "outputs": [],
      "source": [
        "env = Environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQR6upV-0QWS"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EPISODES = 32\n",
        "STEPS = 128\n",
        "\n",
        "agent = Agent(LEARNING_RATE=2.5e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZI7ENg30QWT"
      },
      "outputs": [],
      "source": [
        "# Have robot spin to keep arduino on.\n",
        "agent.wait_for_next_epsiode()\n",
        "for episode in range(EPISODES):\n",
        "    episode_reward = 0\n",
        "    episode_collisions = 0\n",
        "    action_sequence = []\n",
        "    state = env.reset()\n",
        "    for step in range(STEPS):\n",
        "        print(f'\\rStep {step+1}/{STEPS}',end='')\n",
        "        value, policy = agent.call_actor_critic(state)\n",
        "\n",
        "        with torch.autograd.no_grad():\n",
        "        # Select action from policy\n",
        "            action = agent.select_action(policy)\n",
        "\n",
        "        action_prob = policy.view(-1)[action]\n",
        "\n",
        "        # Calculate policy entropy\n",
        "        policy_entropy = -torch.sum(policy.view(-1) * torch.log(policy.view(-1))) \n",
        "\n",
        "        if step == 0:\n",
        "            # Stop robot from spinning.\n",
        "            agent.execute_action(0)\n",
        "        agent.execute_action(action)\n",
        "        action_sequence.append(action)\n",
        "\n",
        "        # Transition to next state\n",
        "        new_state, reward, collision = env.step(action)\n",
        "\n",
        "        episode_collisions += int(collision)\n",
        "        episode_reward += reward\n",
        "        # print(f'Step: {step+1}, Action: {action}, Reward: {reward}, Collided?: {collision}')#, end='')\n",
        "\n",
        "        state = new_state\n",
        "        if 0 < step < STEPS:\n",
        "            with torch.inference_mode():\n",
        "                next_value, _ = agent.call_actor_critic(state)\n",
        "            agent.optimize_policy(action_prob, next_value.view(-1), value.view(-1), reward, policy_entropy)\n",
        "\n",
        "    agent.HISTORY['Returns'].append(episode_reward)\n",
        "    agent.HISTORY['Collisions'].append(episode_collisions)\n",
        "    agent.HISTORY['Actions'].append(action_sequence)\n",
        "    if episode % 5 == 0:\n",
        "        print(f'\\nEpisode {episode+1}/{EPISODES}: Reward {episode_reward}')\n",
        "\n",
        "    # Have robot spin to keep arduino on.\n",
        "    agent.wait_for_next_epsiode()\n",
        "    \n",
        "agent.execute_action(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Do5hmt0QWW"
      },
      "outputs": [],
      "source": [
        "torch.save(agent.HISTORY, 'A2C_History.pth')\n",
        "torch.save(agent.model.state_dict(), 'A2C_Weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsevLDkf0QWX"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "TD-AC.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
