{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import serial\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from CSI_Camera import CSI_Camera\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        self.camera = CSI_Camera(0)\n",
    "        self.time_last_action = 0\n",
    "        self.last_action = None\n",
    "        self.last_frame = None\n",
    "        self.ACTION_MAP = {0:'s', 1:'f', 2:'b'}\n",
    "    def reset(self):\n",
    "        # Check if cameras are opened.\n",
    "        if not self.camera.isRunning() or not self.camera.isOpened():\n",
    "            # Open and start camera\n",
    "            self.camera.open()\n",
    "            self.camera.start()\n",
    "\n",
    "        # Initialize variables\n",
    "        self.time_last_action = 0\n",
    "        self.last_action = None\n",
    "\n",
    "        # Read frame from camera\n",
    "        _, frame = self.camera.read()\n",
    "        # self.last_frame = frame\n",
    "        return self.get_state(frame)\n",
    "    def close(self):\n",
    "        # Stop the camera and close them\n",
    "        if not self.camera.isOpened():\n",
    "            self.stop()\n",
    "            self.release()\n",
    "    def get_state(self, frame):\n",
    "        frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY, dst=None)\n",
    "        frame = cv.resize(frame, (240, 240), dst=None, interpolation=cv.INTER_CUBIC)\n",
    "        state = torch.tensor(frame, device=self.DEVICE).unsqueeze(0)/255\n",
    "        return state\n",
    "    def step(self, action):\n",
    "        try:\n",
    "            action = self.ACTION_MAP[action]\n",
    "            # reset if new action.\n",
    "            if self.last_action != action:\n",
    "                self.time_last_action = 0\n",
    "\n",
    "            # Read new frame from camera\n",
    "            _, frame = self.camera.read()\n",
    "            state = self.get_state(frame)\n",
    "\n",
    "            # Calculate reward of the last (state, action)\n",
    "            reward, colision = self.reward(action, state)\n",
    "\n",
    "            # Accumlate time since executing same action.\n",
    "            # Reset if new action is executed\n",
    "            self.time_last_action += 1\n",
    "            \n",
    "            # Update last_action and last_frame\n",
    "            self.last_action = action\n",
    "            self.last_frame = frame\n",
    "\n",
    "            return state, reward, colision\n",
    "        except RuntimeError:\n",
    "            print(\"Error: Camera not opened.\")\n",
    "            \n",
    "    def reward(self, action, frame):\n",
    "        collision = False\n",
    "        if self.last_action == action:\n",
    "            # Check for collision:\n",
    "            if action in ['f', 'b']:\n",
    "                # Comparing the current frame and the last frame. \n",
    "                # This is done by comparing the histograms of both frames. \n",
    "                # Convert to HSV color space\n",
    "                last_frame_hsv = cv.cvtColor(self.last_frame, cv.COLOR_BGR2HSV)\n",
    "                frame_hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
    "                \n",
    "                # Histogram parameters\n",
    "                hist_size = [50, 60]\n",
    "                ranges = [0, 180, 0, 256]\n",
    "                channels = [0, 1]\n",
    "\n",
    "                # Calculate the Histogram of the last frame and normalize it.\n",
    "                hist_last_frame = cv.calcHist([last_frame_hsv], channels, None, hist_size, ranges, accumulate=False)\n",
    "                cv.normalize(hist_last_frame, hist_last_frame, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
    "                \n",
    "                # Calculate the Histogram of the current frame and normalize it.\n",
    "                hist_frame = cv.calcHist([frame_hsv], channels, None, hist_size, ranges, accumulate=False)\n",
    "                cv.normalize(hist_frame, hist_frame, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
    "\n",
    "                # Calculate the correlation coefficient between the histograms\n",
    "                correlation = cv.compareHist(hist_last_frame, hist_frame, 0)\n",
    "\n",
    "                # If correlation is higher than 0.95 then we can say that it's a collision with a barrier.\n",
    "                if correlation >= 0.95:\n",
    "                    # Collision\n",
    "                    collision = True\n",
    "                    reward = -10\n",
    "        if not collision:\n",
    "            if action == 's':\n",
    "                # Reward for stopping\n",
    "                reward = min(-1, -1 * self.time_last_action)\n",
    "            elif action in ['l', 'r']:\n",
    "                reward = 1\n",
    "            elif action in ['f','b']:\n",
    "                # Going forward or backwards\n",
    "                reward = max(2, 2 * self.time_last_action)\n",
    "        return reward, collision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorLoss, self).__init__()\n",
    "    def forward(self, action_log_probs, advantage):\n",
    "        return torch.mean(-action_log_probs * advantage)\n",
    "        \n",
    "class ActorCriticLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCriticLoss, self).__init__()\n",
    "        self.actor_criterion = ActorLoss()\n",
    "        self.critic_criterion = nn.HuberLoss(reduction='mean')\n",
    "    def forward(self, action_prob, next_value, value, reward, policy_entropy, beta=0.001, discount=0.99):\n",
    "        q_value = (reward + discount * next_value)\n",
    "        advantage = q_value - value\n",
    "        action_log_probs = torch.log(action_prob)\n",
    "\n",
    "        loss_actor = self.actor_criterion(action_log_probs, advantage).to(torch.float)\n",
    "        loss_critic = self.critic_criterion(value, q_value).to(torch.float)\n",
    "\n",
    "        return loss_actor + loss_critic + (policy_entropy * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,out_channels=8,kernel_size=5,stride=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=8,out_channels=8,kernel_size=5,stride=3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(in_channels=8,out_channels=8,kernel_size=3,stride=2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=288, out_features=48),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.Actor = nn.Sequential(\n",
    "            nn.Linear(in_features=48, out_features=num_actions),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.Critic = nn.Sequential(\n",
    "            nn.Linear(in_features=48, out_features=1)\n",
    "        )\n",
    "    def forward(self,state):\n",
    "        features = self.CNN(state)\n",
    "        \n",
    "        value = self.Critic(features)\n",
    "        policy = self.Actor(features)\n",
    "\n",
    "        return value, policy   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('action_prob', 'value', 'reward'))\n",
    "class Memory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self.memory.__iter__\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.memory)\n",
    "\n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, mem_cap, LEARNING_RATE=25e-4):\n",
    "        self.ACTION_MAP = {0:'s', 1:'f', 2:'b', 3:'l', 4:'r'}\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.driver = serial.Serial(\"/dev/ttyTHS1\", 9600, timeout=1)\n",
    "        self.driver.reset_input_buffer()\n",
    "        self.last_action = None\n",
    "\n",
    "        self.model = ActorCritic(3).to(self.DEVICE)\n",
    "        self.optimizer = optim.Adam(params=self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = ActorCriticLoss()\n",
    "        self.memory = Memory(mem_cap)\n",
    "\n",
    "    def select_action(self, policy):\n",
    "        policy_dist = torch.distributions.Categorical(policy.view(-1))\n",
    "        action = policy_dist.sample().item()\n",
    "        return action\n",
    "\n",
    "    def execute_action(self, action):\n",
    "        action = self.ACTION_MAP[action]\n",
    "        if action in ['f','b'] and self.last_action in ['f','b'] and self.last_action != action:\n",
    "            # Have to stop first to go opposite direction\n",
    "            self.driver.write('s'.encode('ascii'))\n",
    "            time.sleep(1)\n",
    "        self.driver.write(action.encode('ascii'))\n",
    "        time.sleep(1)\n",
    "        self.last_action = action\n",
    "\n",
    "    def wait_for_next_epsiode(self):\n",
    "        self.driver.write('s'.encode('ascii'))\n",
    "        time.sleep(1)\n",
    "        self.driver.write('c'.encode('ascii'))\n",
    "    \n",
    "    # def optimize_policy(self, action_prob, next_value, value, reward, policy_entropy):    \n",
    "    #     # Calculate Loss\n",
    "    #     loss = self.criterion(action_prob, next_value, value, reward, policy_entropy)\n",
    "\n",
    "    #     # Backprop & weight update\n",
    "    #     self.optimizer.zero_grad()\n",
    "    #     self.loss.backward()\n",
    "    #     self.optimizer.step()\n",
    "    #     self.optimizer.zero_grad()\n",
    "    def optimize_policy(self, next_value):\n",
    "        for i,(_,_,reward) in enumerate(self.memory):\n",
    "            print(reward)\n",
    "\n",
    "    def add_memory(self, *args):\n",
    "        self.memory.push(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 2\n",
    "STEPS = 2\n",
    "\n",
    "agent = Agent(STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have robot spin to keep arduino on.\n",
    "agent.wait_for_next_epsiode()\n",
    "print('what is going on?')\n",
    "# # policy_entropy = torch.tensor(0, dtype=torch.float, requires_grad=True, device=DEVICE)\n",
    "# returns = []\n",
    "# for episode in range(EPISODES):\n",
    "#     action_sequence = []\n",
    "#     episode_reward = 0\n",
    "\n",
    "#     state = env.reset()\n",
    "#     for step in range(STEPS):\n",
    "#         print(f'\\rStep: {step+1}', end='')\n",
    "#         value, policy = agent.model(state.unsqueeze(0))\n",
    "\n",
    "#         with torch.autograd.no_grad():\n",
    "#         # Select action from policy\n",
    "#             action = agent.select_action(policy)\n",
    "\n",
    "#         action_prob = policy.view(-1)[action]\n",
    "\n",
    "#         # Calculate policy entropy\n",
    "#         # policy_entropy = -torch.sum(policy.view(-1) * torch.log(policy.view(-1))) \n",
    "\n",
    "#         if step == 0:\n",
    "#             # Stop robot from spinning.\n",
    "#             agent.execute_action(0)\n",
    "#         agent.execute_action(action)\n",
    "\n",
    "#         # Transition to next state\n",
    "#         new_state, reward = env.step(action)\n",
    "\n",
    "#         action_sequence.append(action)\n",
    "#         agent.add_memory(action_prob, value, reward)\n",
    "#         episode_reward += reward\n",
    "\n",
    "#         state = new_state\n",
    "#         if 0 < step < STEPS:\n",
    "#             with torch.inference_mode():\n",
    "#                 next_value, _ = agent.model(state.unsqueeze(0))\n",
    "#             # agent.optimize_policy(action_prob, next_value.view(-1), value.view(-1), reward, policy_entropy)\n",
    "#             agent.optimize_policy(next_value)\n",
    "\n",
    "#     returns.append(episode_reward)\n",
    "#     if episode % 1 == 0:\n",
    "#         print(f'\\nEpisode {episode+1}/{EPISODES}: Reward {episode_reward}')\n",
    "\n",
    "#     # Have robot spin to keep arduino on.\n",
    "#     agent.wait_for_next_epsiode()\n",
    "    \n",
    "# agent.execute_action(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
